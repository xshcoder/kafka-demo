services:
  zookeeper:
    build: ./zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD-SHELL", "timeout 2 bash -c 'cat < /dev/null > /dev/tcp/localhost/2181'"]
      interval: 10s
      timeout: 5s
      retries: 3

  kafka:
    build: ./kafka
    container_name: kafka
    ports:
      - "9092:9092"       # Kafka broker port
      - "7071:7071"       # JMX exporter port for Prometheus
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # JMX Exporter Configuration
      KAFKA_OPTS: "-javaagent:/opt/prometheus/jmx_prometheus_javaagent.jar=7071:/opt/prometheus/kafka-broker.yml"
    healthcheck:
      test: ["CMD-SHELL", "echo > /dev/tcp/localhost/9092"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s

  producer:
    build: ./producer
    container_name: producer
    ports:
      - "8080:8080"
      #  - kafka
    environment:
      SPRING_KAFKA_PRODUCER_BOOTSTRAP_SERVERS: kafka:9092
      APP_KAFKA_TOPIC: demo-topic
      APP_KAFKA_STREAM_TOPIC: demo-stream-topic # This line is already present
      SPRING_KAFKA_PRODUCER_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Ensure schema registry URL is passed

  consumer:
    build: ./consumer
    container_name: consumer
    ports:
      - "8081:8080" # Assuming consumer runs on 8080 internally
    depends_on:
      schema-registry:
        condition: service_healthy
    #depends_on:
    #   kafka:
    #     condition: service_healthy
    #   schema-registry: # Added dependency
    #     condition: service_healthy
    environment:
      SPRING_KAFKA_CONSUMER_BOOTSTRAP_SERVERS: kafka:9092
      APP_KAFKA_TOPIC: demo-topic
      APP_KAFKA_STREAM_TOPIC: demo-stream-topic # Added stream topic env var
      SPRING_KAFKA_CONSUMER_GROUP_ID: my-group
      SPRING_KAFKA_CONSUMER_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Added schema registry URL
      SPRING_KAFKA_STREAMS_APPLICATION_ID: post-stream-processor # Added Streams App ID

  # Added Elasticsearch service
  elasticsearch:
    build: ./elasticsearch # Use the Dockerfile in the elasticsearch directory
    container_name: elasticsearch
    ports:
      - "9200:9200" # HTTP port
      - "9300:9300" # Transport port
    environment:
      discovery.type: single-node # Run as a single node
      xpack.security.enabled: "false" # Disable security (for development only!)
      ES_JAVA_OPTS: "-Xms512m -Xmx512m" # Set heap size
    # Optional: Add a volume for data persistence
    # volumes:
    #   - elasticsearch_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200 >/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

# Optional: Define the volume for data persistence
# volumes:
#   elasticsearch_data:
#     driver: local

  # Add Kafka Connect service
  kafka-connect:
    build: ./kafka-connect
    container_name: kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      schema-registry: # Added dependency on schema-registry
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      # Keep essential Kafka Connect environment variables
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_PORT: 8083 # Port for the REST API (often needed, even in standalone)
      CONNECT_GROUP_ID: "kafka-connect-group" # Still relevant for standalone mode offsets
      CONNECT_CONFIG_STORAGE_TOPIC: "kafka-connect-configs" # Topic for config storage (used by standalone)
      CONNECT_OFFSET_STORAGE_TOPIC: "kafka-connect-offsets" # Topic for offset storage (used by standalone)
      CONNECT_STATUS_STORAGE_TOPIC: "kafka-connect-status" # Topic for status storage (used by standalone)
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for internal topics
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for internal topics
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1 # Replication factor for internal topics
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect" # How the REST API advertises itself
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components" # Path to connector
      # Add back the required default worker-level converters
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter" # Add this back
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.protobuf.ProtobufConverter" # Keep or set a suitable default value converter
    # Add command to start in standalone mode with the properties file
    command:
      - /bin/bash
      - -c
      - |
        echo "Waiting for Kafka to be ready..."
        cub kafka-ready -b kafka:9092 1 60
        echo "Waiting for Schema Registry to be ready..."
        cub sr-ready schema-registry 8081 60
        echo "Starting Kafka Connect worker..."
        /etc/confluent/docker/run &
        # Keep the container running
        sleep infinity

  # Add Kibana service
  kibana:
    build: ./kibana
    container_name: kibana
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      ELASTICSEARCH_URL: http://elasticsearch:9200
      XPACK_SECURITY_ENABLED: "false"
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:5601/api/status | grep -q 'Looking good'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Added Schema Registry service
  schema-registry:
    build: ./schema-registry # Use the Dockerfile in the schema-registry directory
    container_name: schema-registry
    ports:
      - "8085:8081" # Map host port 8085 to container port 8081 (default SR port)
    depends_on:
      kafka:
        condition: service_healthy # Wait for Kafka to be healthy
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Add Prometheus service
  prometheus:
    build: ./prometheus
    container_name: prometheus
    ports:
      - "9090:9090" # Expose Prometheus UI port
    depends_on:
      kafka:
        condition: service_healthy # Wait for Kafka to be healthy before scraping
      producer:
        condition: service_started
      consumer:                     # Add dependency on consumer
        condition: service_started  # Or service_healthy if consumer has a healthcheck
    # Optional: Add volumes for Prometheus data persistence
    # volumes:
    #   - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks: # Add this section
      - kafka-demo_default

# Optional: Define volumes if used
# volumes:
#   prometheus_data:
#     driver: local
#   elasticsearch_data: # Keep existing volumes if any
#     driver: local

  grafana:
    build:
      context: ./grafana
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin # Default password, change if needed
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus
    networks:
      - kafka-demo_default

# Ensure the network definition exists, for example:
networks:
  kafka-demo_default:
    driver: bridge